# ì „ë‚¨ ëª¨í…” í…ìŠ¤íŠ¸ë§ˆì´ë‹
# ì „ë‚¨ ëª¨í…”ì •ë³´ json í˜•ì‹ì„ ê°€ì ¸ì™€ì„œ ë¦¬ë·°ë‚´ìš©ì„ ì›Œë“œí´ë¼ìš°ë“œë¡œ ë¶„ì„
# í˜•ìš©ì‚¬ë§Œ í–ˆì§€ë§Œ í…ìŠ¤íŠ¸ë§ˆì´ë‹ ê²°ê³¼ê°€ ë¶€ì¡±í•´ë³´ì—¬ ëª…ì‚¬ë„ ì¶”ê°€
# ../data/jeonnam_motel_words_1018.csv íŒŒì¼ë¡œ ì €ì¥

import pandas as pd
from konlpy.tag import Okt
import streamlit as st
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

st.set_page_config(page_title='Hello, Jeonnam Motel! ğŸ˜‰ ', page_icon='ğŸ˜‰')
st.sidebar.header('Hello, Jeonnam Motel!!')
st.title('ì•¼ë†€ì ì „ë‚¨ ëª¨í…” ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ âœâœ')

# ì—¬ëŸ¬ jsoníŒŒì¼ì˜ ê²½ë¡œ
# json_files = ['../mdata/damyang_ghg_motel.json','../mdata/gwangyang_motel.json','../mdata/haenam_wjkjbg_motel.json','../mdata/mokpo_motel.json','../mdata/mooan_sinan_ya_motel.json','../mdata/najoo_hyj_motel.json','../mdata/sooncheon_motel.json','../mdata/yeosu_motel.json']
json_file = '../data/motel_list_last.json'
data = pd.read_json(json_file)


all_reviews = []   # ëª¨ë“  ë¦¬ë·°ë¥¼ ë‹´ì

reviews_counts = {}   # ë¦¬ë·°ê°œìˆ˜ë¥¼ ì˜ ê°€ì ¸ì™”ë‚˜ ë³´ë ¤ê³  ì“´ ì½”ë“œ


reviews = data['í›„ê¸°'].explode().dropna().tolist()  # `explode`ë¡œ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” í›„ê¸° í¼ì¹˜ê¸°

reviews_counts[json_file] = len(reviews)

all_reviews.extend(reviews)

for json_file, count in reviews_counts.items():
    print(f'From {json_file} : {count} reviews')

# ê° ì§€ì—­ë³„ ìˆ™ì†Œ ëª‡ê°œì¸ì§€ ì¶œë ¥
#print(f'Total reviews in all_reviews: {len(all_reviews)}')

fontpath = 'c:/Windows/Fonts/malgun.ttf'
twitter = Okt()

# stopwords ë¶ˆëŸ¬ì˜¤ê¸°
with open("../data/stopwords-kor.txt", "r", encoding="utf-8") as f:
    stopwords = f.readlines()
stopwords = [word.strip() for word in stopwords]  # ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°

one_char_words = set()  # 1ê¸€ìì§œë¦¬ ë‹¨ì–´ëŠ” stopwordsì— ì €ì¥í•˜ë„ë¡ ì§‘í•©

# í˜•ìš©ì‚¬ ì¶”ì¶œ
# tagged: í•œêµ­ì–´í…ìŠ¤íŠ¸ì—ì„œ í˜•íƒœì†Œ ë¶„ì„ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
def extract_adjectives(text):
    tagged = twitter.pos(text, stem=True)
    words = []
    for word, tag in tagged:
        if tag == 'Adjective':
            if len(word) == 1:    # 1ê¸€ì ë‹¨ì–´ë©´ ì§‘í•©ì— ì¶”ê°€í•˜ê³ , wordsì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.
                one_char_words.add(word)
            elif word not in stopwords:  # 1ê¸€ì ì´ìƒì˜ ë‹¨ì–´ì´ë©´ì„œ ë¶ˆìš©ì–´ì— ì—†ìœ¼ë©´ wordsì— ì¶”ê°€
                words.append(word)
    return words

def extract_nouns(text):
    tagged = twitter.pos(text, stem=True)
    words = []
    for word, tag in tagged:
        if tag == 'Noun':
            if len(word) == 1:    # 1ê¸€ì ë‹¨ì–´ë©´ ì§‘í•©ì— ì¶”ê°€í•˜ê³ , wordsì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.
                one_char_words.add(word)
            elif word not in stopwords:  # 1ê¸€ì ì´ìƒì˜ ë‹¨ì–´ì´ë©´ì„œ ë¶ˆìš©ì–´ì— ì—†ìœ¼ë©´ wordsì— ì¶”ê°€
                words.append(word)
    return words

    # # ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ëŠ” ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ëŠ” ê°€ì ¸ì˜¤ì§€ë§ˆ
    # return [word for word, tag in tagged if tag in ['Noun', 'Adjective'] and word not in stopwords]

all_reviews_text = ' '.join(all_reviews)     # ì „ì²´íŒŒì¼ì—ì„œ ë¦¬ë·°ë‚´ìš© ë¬¸ìì—´ë¡œ ì—°ê²°
all_adj = extract_adjectives(all_reviews_text)    # ê°€ì ¸ì˜¨ ë¬¸ìì—´ì—ì„œ í˜•ìš©ì‚¬ ì¶”ì¶œ
all_noun = extract_nouns(all_reviews_text)    # ê°€ì ¸ì˜¨ ë¬¸ìì—´ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ


counted_adj = Counter(all_adj)   # í˜•ìš©ì‚¬ì¹´ìš´íŠ¸ í•´ì„œ counter_wordsì— ì €ì¥
counted_noun = Counter(all_noun)   # í˜•ìš©ì‚¬ì¹´ìš´íŠ¸ í•´ì„œ counter_wordsì— ì €ì¥

# í˜•ìš©ì‚¬ì™€ ëª…ì‚¬ì˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df_adj = pd.DataFrame(counted_adj.most_common(), columns=['Keyword', 'Frequency'])
df_noun = pd.DataFrame(counted_noun.most_common(), columns=['Keyword', 'Frequency'])

# ë‘ ë°ì´í„°í”„ë ˆì„ì„ í•©ì¹˜ê¸°
merged_df = pd.concat([df_adj, df_noun], axis=0).reset_index(drop=True)

# í•©ì¹œ ë°ì´í„°í”„ë ˆì„ì„ CSVë¡œ ì €ì¥
merged_df.to_csv('../data/jeonnam_motel_2letters.csv', index=False, encoding='utf-8-sig')

# 1ê¸€ì ì§œë¦¬ ë‹¨ì–´ë¥¼ stopwords-kor.txt íŒŒì¼ì— ì¶”ê°€
with open('../data/stopwords-kor.txt', 'a', encoding='utf-8') as f:
    for word in one_char_words:
        f.write(f'\n{word}')

# í˜•ìš©ì‚¬ìš© ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
counted_adjectives = Counter(all_adj)
with st.spinner('ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì¤‘... (í˜•ìš©ì‚¬)'):
    wc_adjectives = WordCloud(font_path=fontpath,
                              background_color="white",
                              width=800,
                              height=600,
                              max_words=10000,
                              max_font_size=100,
                              min_font_size=10
                              ).generate_from_frequencies(counted_adjectives)

    plt.figure(figsize=(10, 8))
    plt.imshow(wc_adjectives, interpolation="bilinear")
    plt.title("í˜•ìš©ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ")
    plt.axis('off')

    st.pyplot(plt)  # Streamlitì— í˜•ìš©ì‚¬ìš© ì›Œë“œí´ë¼ìš°ë“œ ì¶œë ¥


#ëª…ì‚¬ìš© ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
counted_nouns = Counter(all_noun)
with st.spinner('ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì¤‘... (ëª…ì‚¬)'):
    wc_nouns = WordCloud(font_path=fontpath,
                         background_color="white",
                         width=800,
                         height=600,
                         max_words=10000,
                         max_font_size=100,
                         min_font_size=10
                         ).generate_from_frequencies(counted_nouns)

    plt.figure(figsize=(10, 8))
    plt.imshow(wc_nouns, interpolation="bilinear")
    plt.title("ëª…ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ")
    plt.axis('off')
    plt.show()

    st.pyplot(plt)  # Streamlitì— ëª…ì‚¬ìš© ì›Œë“œí´ë¼ìš°ë“œ ì¶œë ¥